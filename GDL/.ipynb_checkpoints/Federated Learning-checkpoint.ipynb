{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951bfc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Federated Learning is about a model going into a secure environment and learning how\n",
    "to solve a problem w/o needing the data to move anywhere. \n",
    "\n",
    "This is huge for security. \n",
    "\"\"\"\n",
    "\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import sys\n",
    "import codecs\n",
    "\n",
    "from toydl import layer\n",
    "from toydl.layer.layer import Embedding, RNNCell, CrossEntropyLoss, MSELoss\n",
    "from toydl.sgd.sgd import SGD\n",
    "from toydl.tensor.tensor import Tensor\n",
    "\n",
    "np.random.seed(12345)\n",
    "\n",
    "ham_ = hashlib.md5(\"ham.txt\".encode(\"utf-8\")).hexdigest()\n",
    "spam_ = hashlib.md5(\"spam.txt\".encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "with codecs.open(ham_, 'r', encoding='utf-8', errors='ignore') as f: \n",
    "    raw_h = f.readlines()\n",
    "with codecs.open(spam_, 'r', encoding=\"utf-8\", errors=\"ignore\") as f: \n",
    "    raw_s = f.readlines()\n",
    "\n",
    "vocab, spam, ham = (set([\"<unk>\"]), list(), list())\n",
    "for row in raw_s:\n",
    "    spam.append(set(row[:-2].split(\" \")))\n",
    "    for word in spam[-1]: \n",
    "        vocab.add(word)\n",
    "\n",
    "for row in raw_h: \n",
    "    ham.append(set(row[:-2].split(\" \")))\n",
    "    for word in ham[-1]: \n",
    "        vocab.add(word)\n",
    "\n",
    "vocab, w2i = (list(vocab), {})\n",
    "for i, w in enumerate(vocab):\n",
    "    w2i[w] = i\n",
    "\n",
    "# here we make a function to pad the emails into 500 word long chunks\n",
    "# we also append and prepend <unk> as a seperator token.\n",
    "def to_indices(input, l=500):\n",
    "    indices = list()\n",
    "    for line in input:\n",
    "        if(len(line) < l):\n",
    "            line = list(line) + [\"<unk>\"] * (l - len(line))\n",
    "            idxs = list()\n",
    "            for word in line: \n",
    "                idxs.append(w2i[word])\n",
    "            indices.append(idxs)\n",
    "    return indices\n",
    "    \n",
    "    \n",
    "spam_idx = to_indices(spam)\n",
    "ham_idx = to_indices(ham)\n",
    "\n",
    "\n",
    "\n",
    "train_spam_idx = spam_idx[0:-1000]\n",
    "train_ham_idx = ham_idx[0:-1000]\n",
    "\n",
    "test_spam_idx = spam_idx[-1000:]\n",
    "test_ham_idx = ham_idx[-1000:]\n",
    "\n",
    "train_data = list()\n",
    "train_target = list()\n",
    "\n",
    "\n",
    "test_data = list()\n",
    "test_target = list()\n",
    "\n",
    "for i in range(max(len(train_spam_idx), len(train_ham_idx))): \n",
    "    train_data.append(train_spam_idx[i%len(train_spam_idx)])\n",
    "    train_target.append([1])\n",
    "    \n",
    "    train_data.append(train_ham_idx[i%len(train_ham_idx)])\n",
    "    train_target.append([0])\n",
    "    \n",
    "for i in range(max(len(test_spam_idx), len(test_ham_idx))): \n",
    "    test_data.append(test_spam_idx[i%len(test_spam_idx)])\n",
    "    test_target.append([1])\n",
    "    \n",
    "    test_data.append(test_ham_idx[i%len(test_ham_idx)])\n",
    "    test_target.append([0])\n",
    "    \n",
    "    \n",
    "def train(model, input_data, target_data, batch_size=500, iterations=5):\n",
    "    n_batches = int(len(input_data) / batch_size)\n",
    "    for iter in range(iterations):\n",
    "        iter_loss = 0\n",
    "        for b_i in range(n_batches):\n",
    "            bs = n_batches\n",
    "            # padding token should stay at 0\n",
    "            model.weight.data[w2i['<unk>']] *= 0\n",
    "            input = Tensor(input_data[b_i*bs:(b_i+1)*bs], autograd=True)\n",
    "            target = Tensor(target_data[b_i*bs:(b_i+1)*bs], autograd=True)\n",
    "            \n",
    "            pred = model.forward(input).sum(1).sigmoid()\n",
    "            loss = criterion.forward(pred, target)\n",
    "            \n",
    "            loss.backward(grad=None)\n",
    "            optim.step()\n",
    "            \n",
    "            iter_loss += loss.data[0] / bs\n",
    "            \n",
    "            sys.stdout.write(\"\\r\\tLoss:\" + str(iter_loss / (b_i+1)))\n",
    "        print()\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, test_input, test_output): \n",
    "    model.weight.data[w2i['<unk>']] *= 0\n",
    "    input = Tensor(test_input, autograd=True)\n",
    "    target = Tensor(test_output, autograd=True)\n",
    "    pred = model.forward(input).sum(1).sigmoid()\n",
    "    return ((pred.data > 0.5) == target.data).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecac2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss:0.06458066488952852\n",
      "% Correct on Test Set: 97.75\n",
      "\tLoss:0.029321291479249295\n",
      "% Correct on Test Set: 98.1\n",
      "\tLoss:0.021904861617501638\n",
      "% Correct on Test Set: 98.3\n"
     ]
    }
   ],
   "source": [
    "# Create our model here\n",
    "model = Embedding(vocab_size=len(vocab), dim=1)\n",
    "model.weight.data *= 0\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.01)\n",
    "\n",
    "for i in range(3):\n",
    "    model = train(model, train_data, train_target, iterations=1)\n",
    "    print(\"% Correct on Test Set: \" + str(test(model, test_data, test_target)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630382f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Round\n",
      "\t Step 1: send the model to Bob\n",
      "\tLoss:0.11674711108092445\n",
      "\n",
      "\t Step 2: send the model to Alica\n",
      "\tLoss:0.0154820568028543057\n",
      "\n",
      "\t Step 3: send the model to Sue\n",
      "\tLoss:0.020925986728137825\n",
      "\n",
      "\tAverage Everyone's New Models\n",
      "\t% Correct on Test Set: 98.3\n",
      "\n",
      "Repeat the process..\n",
      "\n",
      "Starting Training Round\n",
      "\t Step 1: send the model to Bob\n",
      "\tLoss:0.11674711108092445\n",
      "\n",
      "\t Step 2: send the model to Alica\n",
      "\tLoss:0.015482056802854308\n",
      "\n",
      "\t Step 3: send the model to Sue\n",
      "\tLoss:0.020925986728137825\n",
      "\n",
      "\tAverage Everyone's New Models\n",
      "\t% Correct on Test Set: 98.3\n",
      "\n",
      "Repeat the process..\n",
      "\n",
      "Starting Training Round\n",
      "\t Step 1: send the model to Bob\n",
      "\tLoss:0.11674711108092445\n",
      "\n",
      "\t Step 2: send the model to Alica\n",
      "\tLoss:0.015482056802854308\n",
      "\n",
      "\t Step 3: send the model to Sue\n",
      "\tLoss:0.020925986728137825\n",
      "\n",
      "\tAverage Everyone's New Models\n",
      "\t% Correct on Test Set: 98.3\n",
      "\n",
      "Repeat the process..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Federating the Data Sets\n",
    "\n",
    "Each person's dataset changes the model slightly.\n",
    "\"\"\"\n",
    "bob = (train_data[0:1000], train_target[0:1000])\n",
    "alice = (train_data[1000:2000], train_target[1000:2000])\n",
    "sue = (train_data[2000:], train_target[2000:])\n",
    "import copy\n",
    "for i in range(3):\n",
    "    print(\"Starting Training Round\")\n",
    "    print(\"\\t Step 1: send the model to Bob\")\n",
    "    bob_model = train(copy.deepcopy(model), bob[0], bob[1], iterations=1)\n",
    "    \n",
    "    print(\"\\n\\t Step 2: send the model to Alica\")\n",
    "    alice_model = train(copy.deepcopy(model), alice[0], alice[1], iterations=1)\n",
    "    \n",
    "    print(\"\\n\\t Step 3: send the model to Sue\")\n",
    "    sue_model = train(copy.deepcopy(model), sue[0], sue[1], iterations=1)\n",
    "    \n",
    "    print(\"\\n\\tAverage Everyone's New Models\")\n",
    "    model.weight.data = (bob_model.weight.data + \\\n",
    "                         alice_model.weight.data + \\\n",
    "                         sue_model.weight.data)/3\n",
    "    \n",
    "    print(\"\\t% Correct on Test Set: \" + \\\n",
    "           str(test(model, test_data, test_target)*100))\n",
    "    \n",
    "    print(\"\\nRepeat the process..\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e484ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Collecting phe\n",
      "  Downloading phe-1.5.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: phe\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Successfully installed phe-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The problem stated above  - we can still backtrack and calculate a single\n",
    "person's gradient. The more noise we add the more the data becomes obfuscated.\n",
    "\n",
    "The only problem w/ this is that it hurts training. \n",
    "\n",
    "We use homomorphic encryption to solve the problem ~ we sum all the gradients\n",
    "from all the participants in sucha way taht no one can see anyone's gradient but \n",
    "their own.\n",
    "\"\"\"\n",
    "!pip3 install phe\n",
    "import phe\n",
    "pub, pri = phe.generate_paillier_keypair(n_length=1024)\n",
    "x = pub.encrypt(5)\n",
    "y = pub.encrypt(4)\n",
    "z = x+y\n",
    "z_ = pri.decrypt(z)\n",
    "print(z_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f3366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
